# Unsupervised Learning and Principle Component Analysis

As we discussed in previous articles, the Hebbian learning rule can be summarized by this equation:

<img src="https://tex.s2cms.ru/svg/w_%7Bji%7D(t%2B1)%20%3D%20w_%7Bji%7D(t)%20%2B%20%5Cepsilon%20x_%7Bj%7D(t)%20y_%7Bi%7D(t)" alt="w_{ji}(t+1) = w_{ji}(t) + \epsilon x_{j}(t) y_{i}(t)" />

Basically, according to the Hebbian learning rule, weights are updated when there is a concurrent activity in the pre-synaptic and post-synaptic neurons.

So, what capabilities does Hebbian learning give us?

1. It allows us to detect correlations [^1] between inputs and outputs (of course!-- "Neurons that fire together wire together")
2. It allows us to detect correlations between different inputs


[^1]: This is the first footnote.










